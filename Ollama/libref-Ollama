#!/usr/bin/env bash

source /etc/AIInAAL/AIInAAL_path
aiinaalpkg=Ollama

AIInAAL_update_Ollama () {
  cd $AIInAALdir/$aiinaalpkg
  git fetch --all
  git branch backup-master
  git reset --hard origin/master
  git pull
  rm ./libref-$aiinaalpkg*
  rm ./requirements_$aiinaalpkg*
  wget https://raw.githubusercontent.com/JT-Gresham/AIInAAL/main/Ollama/libref-$aiinaalpkg
  wget https://raw.githubusercontent.com/JT-Gresham/AIInAAL/main/Ollama/user_customize_$aiinaalpkg_example.sh
  wget https://raw.githubusercontent.com/JT-Gresham/AIInAAL/main/Ollama/requirements_$aiinaalpkg.txt
  pip install -r requirements_$aiinaalpkg.txt

#  if grep -Fxq "import intel_extension_for_pytorch as ipex" webui.py
#    then
#      echo "Intel extensions found in webui.py...skipping"
#    else
#      sed -i 's|from modules import timer|import torch\nimport intel_extension_for_pytorch as ipex\nimport intel_extension_for_transformers\n\nfrom modules import timer|g' webui.py
#  fi

ln -sf "$AIInAALdir/$aiinaalpkg/.ollama/models/blobs"
ln -sf "$AIInAALdir/shared/LLMs/" "$AIInAALdir/$aiinaalpkg/.ollama/models/blobs"

cd $pdirectory
$AIInAALdir/$aiinaalpkg/user_customize_$aiinaalpkg.sh
}

Ollama-exit () {
    kill $(pidof python $AIInAALdir/$aiinaalpkg)
}